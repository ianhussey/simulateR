---
title: "simulateR vignette"
author: "Ian Hussey"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulateR vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r include=FALSE}

# formatting options
# set default chunk options
knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

# disable scientific notation
options(scipen = 999) 

```

# Notation

The {simulateR} package makes frequent use of the variable labels Y, X, and M to (a) describe the data generating model used to simulate data and (b) to describe the analytic model applied to a simulated dataset, so it's useful to understand the usage of these three labels. E.g. analyses:

- *t*-test: $Y \sim X$, where $Y$ is the continuous DV and $X$ is categorical IV (intervention vs. control). 
- Correlation: $Y \sim X$, where $Y$ and $X$ are continuous variables. 
- Regression: $Y \sim X + M$, where $Y$ is the continuous DV and $X$ and $M$ are continuous IVs. E.g., to estimate the causal impact of $X$ on $Y$, while controlling for the covariate $M$.

The second IV is labelled M because this borrows from conventions in mediation analysis:

- Mediation analysis: $Y \sim X + M; M \sim X$, where $Y$ is the continuous DV, $X$ is a continuous IV, and $M$ is a continuous mediating variable. 

{simulateR} also allows these three variables can also (a) generate data from and (b) fit analytic models assuming other configuration models too (e.g., confounds and colliders). For an introduction to understanding the differences between these simple regression/mediation/confounds/colliders, and the importance of thinking about causal modelling, see [Rohrer (2018)](https://journals.sagepub.com/doi/full/10.1177/2515245917745629) and [Wysocki et al. (2022)](https://journals.sagepub.com/doi/full/10.1177/25152459221095823).

# Dependencies & options

```{r}

# dependencies
# devtools::install_github("ianhussey/simulateR") # to install
library(simulateR)
library(furrr)
library(future)
library(tibble)
library(dplyr)
library(tidyr)
library(metafor)
library(knitr)
library(kableExtra)

# set up parallel processing
# without this functions such as data_preprocessing(), data_preprocessing(), 
# and fit_model() will run on a single core. this may be much slower.
future::plan(multicore)

# note that plan(multicore) doesn't work on microsoft windows OS, only on apple mac OS + linux.
# if you are running on a windows machine, you can use the following instead to at least run 
# this code in a second background session of R, which allows you to do other things in R while code is running. 
# future::plan(multisession)

# set seed
set.seed(42)

```

# Dance of the *p* values

- Variable sample sizes between studies

```{r fig.height=5, fig.width=7}

set.seed(42)

true_population_effect_size <- 0

# run a simulation
results <- 
  generate_data(pop_model_label = "ttest indicators",
                pop_model = paste0("Y ~ ", true_population_effect_size, "*X"), 
                factorial_design = TRUE,
                n_mean = 30,
                n_sd = 12,
                iterations = 10) |>
  fit_model(analysis = analysis_ttest) |>
  extract_cohens_d_effect_sizes() |>
  publication_bias(p_pub_sig = 0.90, p_pub_nonsig = 0.10)

# print p values
results |>
  unnest(fit) |>
  select(iteration, `p value` = Y_X_pvalue) |>
  round_df(2) |>
  mutate(`p value` = str_replace(as.character(`p value`), "0.", ".")) |>
  kable() |>
  kable_classic(full_width = FALSE)

# fit meta-analysis
fit <- metaanalysis(results)

# forest plot
forest(fit, 
       xlab = expression("Cohen's"~italic(d)),
       addfit = FALSE,
       addcred = TRUE,
       refline = 0,
       at = c(-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0),
       xlim = c(-3.00, 3.50))

```

# debugging

## latents only, t test

```{r}

set.seed(42)

true_population_effect_size <- 0.35

population_model <- paste0("Y ~ ", true_population_effect_size, "*X")

# population_model <-
#   create_population_model_with_random_item_loadings(
#     model_specification = paste0("Y_latent ~ ", true_population_effect_size, "*X_latent"),
#     item_loading_min_y = 0.5,
#     item_loading_max_y = 0.9,
#     n_indicators_y = 8,
#   )

# run a simulation
results <- 
  generate_data(pop_model_label = "ttest latent only",
                pop_model = population_model, 
                factorial_design = TRUE,
                n = 1000,
                iterations = 1000) |>
  #data_preprocessing(method = convert_to_likert) |>
  #data_processing(method = use_latent_scores) |>
  fit_model(analysis = analysis_ttest) |>
  extract_cohens_d_effect_sizes() |>
  publication_bias(p_pub_sig = 0.90, p_pub_nonsig = 0.10)

metaanalysis(results)

# results |>
#   unnest(fit) |>
#   unnest(effect_sizes) |>
#   select(iteration, Y_X_estimate, Y_X_std_es_estimate, published, y) |>
#   # colnames() |>
#   # dput() 
#   summarize(mean_Y_X_estimate = mean(Y_X_estimate),
#             mean_Y_X_std_es_estimate = mean(Y_X_std_es_estimate),
#             mean_y = mean(y))

```

## latents only, ols regression

```{r}

set.seed(42)

true_population_effect_size <- .80

population_model <- paste0("Y ~ ", true_population_effect_size, "*X")

# population_model <-
#   create_population_model_with_random_item_loadings(
#     model_specification = paste0("Y_latent ~ ", true_population_effect_size, "*X_latent"),
#     item_loading_min_y = 0.5,
#     item_loading_max_y = 0.9,
#     n_indicators_y = 8,
#   )

# run a simulation
results <- 
  generate_data(pop_model_label = "simple regression latent only",
                pop_model = population_model, 
                factorial_design = FALSE,
                #n_mean = 1000,
                #n_sd = 12,
                n = 1000,
                iterations = 1) |>
  #data_preprocessing(method = convert_to_likert) |>
  #data_processing(method = use_latent_scores) |>
  #fit_model(analysis = analysis_regression_ols) |>
  fit_model(analysis = analysis_regression_ml) |>
  #extract_cohens_d_effect_sizes() |>
  publication_bias(p_pub_sig = 0.90, p_pub_nonsig = 0.10)

#lm(Y ~ X, data = results$data_processed[[1]])

# res <- results |>
#   unnest(fit) |>
#   select(iteration, beta = Y_X_estimate, `p value` = Y_X_pvalue, published) |>
#   round_df(3)
#
# res |>
#   mutate(`p value` = str_replace(as.character(`p value`), "0.", ".")) |>
#   kable() |>
#   kable_classic(full_width = FALSE)

results |>
  unnest(fit) |>
  summarize(mean_beta = mean(Y_X_estimate))

```

# Influence of publication bias on meta-analysis

- Variable sample sizes between studies
- Significant results more likely to be published
- Meta-analysis across studies

```{r}

set.seed(42)

true_population_effect_size <- 0.2

# population model
# population_model <-
#   create_population_model_with_random_item_loadings(
#     model_specification = paste0("Y_latent ~ ", true_population_effect_size, "*X_latent"),
#     item_loading_min_y = 0.5,
#     item_loading_max_y = 0.9,
#     n_indicators_y = 8,
#   )

population_model <-
  create_population_model_with_static_item_loadings(
    model_specification = paste0("Y_latent ~ ", true_population_effect_size, "*X_latent")
  )

# run a simulation
results <- 
  generate_data(pop_model_label = "ttest indicators",
                pop_model = population_model, 
                factorial_design = TRUE,
                n_mean = 100,
                n_sd = 30,
                iterations = 100) |>
  data_preprocessing(method = convert_to_likert) |>
  data_processing(method = use_latent_scores) |>
  fit_model(analysis = analysis_ttest) |>
  extract_cohens_d_effect_sizes() |>
  publication_bias(p_pub_sig = 0.90, p_pub_nonsig = 0.10)


# meta with no publication bias
fit_unbiased <- metaanalysis(results)

# meta with publication bias
fit_biased <- metaanalysis(results, published_only = TRUE)

# meta ESs
tibble(label = c("true population", 
                 "meta (all studies)", 
                 "meta (publication biased)"),
       effect_size = c(true_population_effect_size, 
                       janitor::round_half_up(fit_unbiased$beta[,1], 2),
                       janitor::round_half_up(fit_biased$beta[,1], 2)))

```

```{r fig.height=20, fig.width=7}

forest(fit_unbiased, 
       xlab = expression("Cohen's"~italic(d)),
       addcred = TRUE,
       refline = 0,
       at = c(-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5),
       xlim = c(-2.75, 2.75),
       slab = paste("Conducted study", seq_len(fit_unbiased$k)))

```

```{r fig.height=8, fig.width=7}

forest(fit_biased,
       xlab = expression("Cohen's"~italic(d)),
       addcred = TRUE,
       refline = 0,
       at = c(-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5),
       xlim = c(-2.75, 2.75),
       slab = paste("Published study", seq_len(fit_biased$k)))

```

# Continuous IVs

```{r fig.height=4, fig.width=6}

# population models
population_model <-
  create_population_model_with_static_item_loadings(
    model_specification = 
      "Y_latent ~ 0.5*X_latent + 0.5*M_latent; 
       X_latent ~~ 0.5*M_latent",
    item_loading_y = 0.8,
    item_loading_x = 0.7,
    item_loading_m = 0.6,
    n_indicators_y = 10,
    n_indicators_x = 8,
    n_indicators_m = 6
  )

# run a simulation
results <- 
  generate_data(pop_model_label = "covariate indicators",
                pop_model = population_model, 
                n = 100, 
                iterations = 15) |>
  data_preprocessing(method = convert_to_likert) |>
  data_processing(method = calculate_mean_scores) |>
  fit_model(analysis = analysis_covariate) 

|>
  extract_effect_sizes(effect_size = "Y ~ X beta")


# summarize results across simulations
# summarize_mean_correlations(results) # no longer automatically run
summarize_mean_betas(results)
summarize_decision_rate(results)

fit_meta <- results |>
  unnest(effect_sizes) |>
  rma(yi = y,
      sei = se,
      data = _) 

fit_meta

# plot
fit_meta %>%
  forest(., 
         # covariate:
         xlab = bquote(beta[x] ~ " in " ~ Y == beta[I] + beta[x] * X + beta[m] * M),
         # # mediation:
         # xlab = bquote(beta[m] + beta[x2] ~ " in " ~ Y == beta[I] + beta[x1] * X + beta[m] * M ~ "; " ~ M == beta[I] + beta[x2] * X),
         addcred = TRUE,
         mlab = heterogeneity_metrics_for_forest(.),
         refline = 0,
         at = c(-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5),
         xlim = c(-2.75, 2.75))

```

# Session info

```{r}

sessionInfo()

```


