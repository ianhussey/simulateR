% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/publication_bias.R
\name{publication_bias}
\alias{publication_bias}
\title{publication_bias}
\usage{
publication_bias(effect_sizes, p_pub_sig = 0.61, p_pub_nonsig = 0.22)
}
\arguments{
\item{effect_sizes}{nested data frame with results}

\item{p_pub_sig}{probability of a significant result being labelled published = TRUE}

\item{p_pub_nonsig}{probability of a non-significant result being labelled published = TRUE}
}
\value{
A data frame or tibble.
}
\description{
Create a boolean "published" column for each iteration (ie simulated study) based on the prior probability of statistically (non)significant results being published.
}
\details{
This requires that we choose values for the probability of publishing results given significant results and given non significant results. Several estimates of the prevalence of significant vs non-significant results in the literature exist (e.g., Motyl et al., 2017; Sterling et al, 1995) but these estimate estimate the probability of being significant given having been published (i.e., P(significant | published) ).

Here we're interested in the opposite, P(published | significant) and P(published | nonsignificant). Anne Scheel suggests that Franco et al. (2014; 2016) provide estimates for these using a registered database of studies. However, these studies came from a registered database of studies that had been peer reviewed and preapproved prior to be run. There is a chance they are not representative, as they are therefore half-way to being a registered report, which may inflate the rate of published non-significant results. But in the absence of other data, these estimates are informative.

From Franco et al. (2014; 2016):

P(published | significant) = 57/93 = 0.61
P(published | nonsignificant) = 11/49 = 0.22

I use these as default values, but you can use your own. More extreme values might be more realistic, based on anecdotal experience (e.g., p_pub_sig = 0.70, p_pub_nonsig = 0.05)
}
